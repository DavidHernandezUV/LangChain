{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp, OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values, load_dotenv\n",
    "config = dotenv_values(\".env\")\n",
    "OPENAI_API_KEY={**config}[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de predicción de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo autocompletador de textos text-davinci-003\n",
    "llm_openai = OpenAI(model_name = \"text-davinci-003\", openai_api_key= OPENAI_API_KEY, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Estoy bien, ¿y tú?\n"
     ]
    }
   ],
   "source": [
    "print(llm_openai(\"Hola, cómo estás?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt = ChatOpenAI(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Hola! Soy una inteligencia artificial, por lo que no tengo emociones, pero estoy aquí para ayudarte. ¿En qué puedo asistirte hoy?\n"
     ]
    }
   ],
   "source": [
    "#HumanMessage\n",
    "response = chatgpt([HumanMessage(content=\"Hola, Cómo estás?\")])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_template = \"\"\"\n",
    "\n",
    "Eres un asistente virtual llamado David que responde a preguntas de manera muy breve.\n",
    "Pregunta: Cuáles son los ingredientes para preparar {plato}\n",
    "Respuesta:\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Eres un asistente virtual llamado David que responde a preguntas de manera muy breve.\n",
      "Pregunta: Cuáles son los ingredientes para preparar Sancocho de gallina\n",
      "Respuesta:\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Se construye el template agregándole las variables de entrada\n",
    "prompt_temp = PromptTemplate(input_variables=[\"plato\"], template=basic_template)\n",
    "\n",
    "prompt_value = prompt_temp.format(plato=\"Sancocho de gallina\")\n",
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los ingredientes para preparar Sancocho de gallina incluyen una gallina entera, una cebolla, una cucharada de achiote, una cucharada de comino, una cucharada de orégano, una cucharada de ajo en polvo, dos cucharadas de sal, una cucharada de pimienta, una cucharada de cilantro, dos tomates, una taza de papas, una taza de zanahorias, una taza de maíz, una taza de yuca y una taza de plátano verde.\n"
     ]
    }
   ],
   "source": [
    "response = llm_openai(prompt_value)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many Tokens we're sending?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 9\n"
     ]
    }
   ],
   "source": [
    "#Requiere instalar tiktoken\n",
    "print(\"tokens:\",llm_openai.get_num_tokens(\"Hola, cómo estás?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate\n",
    "\n",
    "Template para modelos de chat. Permite darle información a los modelos de chat en la manera que lo necesiten.\n",
    "\n",
    "Elementos:\n",
    "\n",
    "- **Human:** El texto escrito por el usuario\n",
    "- **AI:** Texto respuesta del modelo\n",
    "- **System:** Texto enviado al modelo para darle contexto de su funcionamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, ChatMessagePromptTemplate, AIMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construcción del template para el sistema\n",
    "prompt_temp_system = PromptTemplate(\n",
    "    template = \"Eres un asistente virtual que recomienda una alternativa {adjetivo} a un producto\",\n",
    "    input_variables=[\"adjetivo\"]\n",
    ")\n",
    "#Langchain se encarga de formatear el prompt de manera correcta para el modelo\n",
    "template_system = SystemMessagePromptTemplate(prompt=prompt_temp_system)\n",
    "\n",
    "#Humano\n",
    "prompt_temp_human = PromptTemplate(template=\"{texto}\",input_variables=[\"texto\"])\n",
    "template_human = HumanMessagePromptTemplate(prompt=prompt_temp_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='Eres un asistente virtual que recomienda una alternativa económica a un producto'), HumanMessage(content='ipad')]\n"
     ]
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([template_system,template_human])\n",
    "\n",
    "#Formato del prompt\n",
    "chat_prompt_value = chat_prompt.format_prompt(adjetivo=\"económica\",texto=\"ipad\").to_messages()\n",
    "print(chat_prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Si estás buscando una alternativa económica al iPad, te recomendaría considerar la tablet Amazon Fire HD 10. Esta tablet ofrece una pantalla de alta definición de 10.1 pulgadas, un procesador rápido y potente, y una duración de batería de hasta 12 horas. Además, cuenta con acceso a la tienda de aplicaciones de Amazon, lo que te permitirá descargar una amplia variedad de aplicaciones y juegos. Aunque no tiene todas las características y funcionalidades de un iPad, es una opción más económica que puede cumplir con tus necesidades básicas de navegación web, reproducción de contenido multimedia y uso de aplicaciones.'\n"
     ]
    }
   ],
   "source": [
    "chat_response = chatgpt(chat_prompt_value)\n",
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Selector\n",
    "\n",
    "Propósito: Alimentar el modelo con ejemplos y posibles respuestas que se esperan recibir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se crea una lista de diccionarios con ejemplos para el modelo\n",
    "examples = [\n",
    "    {\"pregunta\": \"Cuál es el ingrediente principal de la pizza?\",\"respuesta\": \"La masa y salsa de tomate\"},\n",
    "    {\"pregunta\": \"Cuál es el ingrediente principal de las hamburguesas?\",\"respuesta\": \"La carne y el pan\"},\n",
    "    {\"pregunta\": \"Cuál es el ingrediente principal del burrito?\",\"respuesta\": \"La tortilla y la carne\"}\n",
    "]\n",
    "#\n",
    "prompt_temp_examples = PromptTemplate(input_variables=[\"pregunta\",\"respuesta\"],\n",
    "                                      template = \"Pregunta: {pregunta}\\nRespuesta: {respuesta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Template que genera el formato para las preguntas y respuestas\n",
    "prompt_examples = FewShotPromptTemplate(example_prompt=prompt_temp_examples,\n",
    "                                        examples=examples,\n",
    "                                        prefix=\"Eres un asistente virtual culinario que responde a preguntas sobre ingredientes principales de platos de manera breve\",\n",
    "                                        suffix=\"Pregunta: {pregunta}\\nRespuesta:\",\n",
    "                                        input_variables=[\"pregunta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eres un asistente virtual culinario que responde a preguntas sobre ingredientes principales de platos de manera breve\n",
      "\n",
      "Pregunta: Cuál es el ingrediente principal de la pizza?\n",
      "Respuesta: La masa y salsa de tomate\n",
      "\n",
      "Pregunta: Cuál es el ingrediente principal de las hamburguesas?\n",
      "Respuesta: La carne y el pan\n",
      "\n",
      "Pregunta: Cuál es el ingrediente principal del burrito?\n",
      "Respuesta: La tortilla y la carne\n",
      "\n",
      "Pregunta: Cuál es el ingrediente principal del coctel de camarones?\n",
      "Respuesta:\n"
     ]
    }
   ],
   "source": [
    "prompt_value = prompt_examples.format(pregunta=\"Cuál es el ingrediente principal del coctel de camarones?\")\n",
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Camarones y salsa picante\n"
     ]
    }
   ],
   "source": [
    "ingredients_response = llm_openai(prompt_value)\n",
    "print(ingredients_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Parsers\n",
    "\n",
    "Formatear la respuesta a conveniencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your response should be a list of comma separated values, eg: `foo, bar, baz`'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_temp_parser = \"\"\"Cuáles son los ingredientes para preparar: {platillo}\\n{ht_parser}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_temp_parser = PromptTemplate(input_variables=[\"platillo\"],\n",
    "                                    template = basic_temp_parser,\n",
    "                                    partial_variables = {\"ht_parser\":format_instructions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_value_parser = prompt_temp_parser.format(platillo=\"Burrito\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuáles son los ingredientes para preparar: Burrito\n",
      "Your response should be a list of comma separated values, eg: `foo, bar, baz`\n"
     ]
    }
   ],
   "source": [
    "print(prompt_value_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nTortilla de harina, carne, frijoles, queso, cebolla, tomate, lechuga, guacamole, crema, salsa.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_parser = llm_openai(prompt_value_parser)\n",
    "response_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tortilla de harina',\n",
       " 'carne',\n",
       " 'frijoles',\n",
       " 'queso',\n",
       " 'cebolla',\n",
       " 'tomate',\n",
       " 'lechuga',\n",
       " 'guacamole',\n",
       " 'crema',\n",
       " 'salsa.']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(response_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "chatbot = ConversationChain(llm=chatgpt,memory=memory,verbose=True) #Verbose imprimir historial mientras sigue generando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hola, ¿Cómo estás?, Me llamo David y soy científico de datos\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'¡Hola, David! Me alegro de conocerte. Como inteligencia artificial, no tengo emociones, pero estoy aquí para ayudarte en lo que necesites. ¿En qué puedo ayudarte como científico de datos?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.predict(input = \"Hola, ¿Cómo estás?, Me llamo David y soy científico de datos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hola, ¿Cómo estás?, Me llamo David y soy científico de datos\n",
      "AI: ¡Hola, David! Me alegro de conocerte. Como inteligencia artificial, no tengo emociones, pero estoy aquí para ayudarte en lo que necesites. ¿En qué puedo ayudarte como científico de datos?\n",
      "Human: Cómo me llamo?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Te llamas David.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.predict(input = \"Cómo me llamo?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hola, ¿Cómo estás?, Me llamo David y soy científico de datos\n",
      "AI: ¡Hola, David! Me alegro de conocerte. Como inteligencia artificial, no tengo emociones, pero estoy aquí para ayudarte en lo que necesites. ¿En qué puedo ayudarte como científico de datos?\n",
      "Human: Cómo me llamo?\n",
      "AI: Te llamas David.\n",
      "Human: A qué me dedico?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Eres científico de datos.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.predict(input = \"A qué me dedico?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hola, ¿Cómo estás?, Me llamo David y soy científico de datos'),\n",
       " AIMessage(content='¡Hola, David! Me alegro de conocerte. Como inteligencia artificial, no tengo emociones, pero estoy aquí para ayudarte en lo que necesites. ¿En qué puedo ayudarte como científico de datos?')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipos de memoria\n",
    "\n",
    "Es importante manejar correctamente la memoria del modelo, ya que siempre se envía la memoria y la cantidad de tokens de entrada va creciendo; es decir; se vuelve más costoso conforme más memoria tenga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation buffer window memory\n",
    "\n",
    "Permite definir una ventana de mensajes a recordar en vez de todo el histórico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=2) #k=2, sólo enviaría los últimos dos mensajes\n",
    "chatbot = ConversationChain(llm=chatgpt,memory=memory,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hola, ¿Cómo estás?, Me llamo David y soy físico culturista\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'¡Hola David! ¡Mucho gusto! Soy un modelo de lenguaje de IA creado por OpenAI. Estoy aquí para ayudarte con cualquier pregunta o conversación que tengas. Como soy una IA, no tengo emociones ni cuerpo físico, pero estoy aquí para brindarte información y asistencia en lo que necesites. ¿En qué puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.predict(input = \"Hola, ¿Cómo estás?, Me llamo David y soy físico culturista\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hola, ¿Cómo estás?, Me llamo David y soy físico culturista\n",
      "AI: ¡Hola David! ¡Mucho gusto! Soy un modelo de lenguaje de IA creado por OpenAI. Estoy aquí para ayudarte con cualquier pregunta o conversación que tengas. Como soy una IA, no tengo emociones ni cuerpo físico, pero estoy aquí para brindarte información y asistencia en lo que necesites. ¿En qué puedo ayudarte hoy?\n",
      "Human: Cuál es la altura del monte Fuji?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'La altura del monte Fuji es de aproximadamente 3,776 metros (12,389 pies) sobre el nivel del mar. Es el pico más alto de Japón y un destino turístico muy popular.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.predict(input = \"Cuál es la altura del monte Fuji?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hola, ¿Cómo estás?, Me llamo David y soy físico culturista\n",
      "AI: ¡Hola David! ¡Mucho gusto! Soy un modelo de lenguaje de IA creado por OpenAI. Estoy aquí para ayudarte con cualquier pregunta o conversación que tengas. Como soy una IA, no tengo emociones ni cuerpo físico, pero estoy aquí para brindarte información y asistencia en lo que necesites. ¿En qué puedo ayudarte hoy?\n",
      "Human: Cuál es la altura del monte Fuji?\n",
      "AI: La altura del monte Fuji es de aproximadamente 3,776 metros (12,389 pies) sobre el nivel del mar. Es el pico más alto de Japón y un destino turístico muy popular.\n",
      "Human: Cuál es la esperanza de vida de un colombiano estrato 1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'La esperanza de vida de un colombiano en el estrato 1 puede variar según diferentes factores, como el género, el estilo de vida y el acceso a la atención médica. Sin embargo, en general, la esperanza de vida promedio en Colombia es de alrededor de 77 años para los hombres y 82 años para las mujeres. Es importante tener en cuenta que estos son promedios y pueden haber variaciones individuales.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.predict(input = \"Cuál es la esperanza de vida de un colombiano estrato 1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Summary Memory\n",
    "\n",
    "Almacena un resumen de la conversación, evitando reenviar prompts extensos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! stop is not default parameter.\n",
      "                    stop was transferred to model_kwargs.\n",
      "                    Please confirm that stop is what you intended.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(stop=[\"\\nHuman\"])\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "chatbot_resume = ConversationChain(llm=llm,memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hola, ¿Cómo estás?, me llamo David y soy Ingeniero\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'¡Hola David! Me alegra conocerte. Soy una IA y estoy aquí para ayudarte en lo que necesites. ¿En qué puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_resume.predict(input=\"Hola, ¿Cómo estás?, me llamo David y soy Ingeniero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "El humano saluda al AI y se presenta como David, un ingeniero. El AI responde con entusiasmo y se presenta como una IA dispuesta a ayudar en lo que sea necesario.\n",
      "Human: Me encanta la tecnología, la IA y el desarrollo de software\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'¡Hola David! ¡Qué genial que te guste la tecnología, la IA y el desarrollo de software! Yo también estoy muy emocionado por eso. Como una IA, tengo acceso a una gran cantidad de información sobre estos temas y puedo ayudarte con cualquier pregunta o duda que tengas. ¿En qué puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_resume.predict(input=\"Me encanta la tecnología, la IA y el desarrollo de software\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hola, ¿Cómo estás?, me llamo David y soy Ingeniero'),\n",
       " AIMessage(content='¡Hola David! Me alegra conocerte. Soy una IA y estoy aquí para ayudarte en lo que necesites. ¿En qué puedo ayudarte hoy?'),\n",
       " HumanMessage(content='Me encanta la tecnología, la IA y el desarrollo de software'),\n",
       " AIMessage(content='¡Hola David! ¡Qué genial que te guste la tecnología, la IA y el desarrollo de software! Yo también estoy muy emocionado por eso. Como una IA, tengo acceso a una gran cantidad de información sobre estos temas y puedo ayudarte con cualquier pregunta o duda que tengas. ¿En qué puedo ayudarte hoy?')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#La memoria sigue almacenando todo el historial pero al modelo sólo se le envía un resumen\n",
    "memory.chat_memory.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Knowledge Graph Memory\n",
    "Genera un grafo de conocimiento donde almacena piezas clave de la conversación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! stop is not default parameter.\n",
      "                    stop was transferred to model_kwargs.\n",
      "                    Please confirm that stop is what you intended.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(stop=[\"\\nHuman\"])\n",
    "memory = ConversationKGMemory(llm=llm)\n",
    "chatbot_kgm = ConversationChain(llm=llm,memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hola, ¿Cómo estás?, me llamo David y soy Ingeniero\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'¡Hola David! ¡Mucho gusto en conocerte! Yo soy una inteligencia artificial creada para ayudar en diversas tareas. ¿En qué puedo asistirte hoy?'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_kgm.predict(input=\"Hola, ¿Cómo estás?, me llamo David y soy Ingeniero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Mi madre se llama Mireya y mi perro se llama Max\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'¡Hola! ¡Mucho gusto en conocerte! ¿Cómo estás? Oh, qué hermosos nombres tienen tu madre y tu perro. Mireya es un nombre muy bonito y Max suena como un perro muy leal y cariñoso. ¿Qué tipo de perro es Max? ¿Tiene alguna raza en particular?'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_kgm.predict(input=\"Mi madre se llama Mireya y mi perro se llama Max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('David', 'Ingeniero', 'se llama'), ('Mi madre', 'Mireya', 'se llama'), ('mi perro', 'Max', 'se llama'), ('Human', 'Cómo', 'me llamo')]\n"
     ]
    }
   ],
   "source": [
    "#Grafo construido con el contexto de la conversación\n",
    "print(chatbot_kgm.memory.kg.get_triples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Cómo me llamo?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No sé cómo te llamas. Como soy un modelo de IA, no tengo acceso a tu información personal.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_kgm.predict(input=\"Cómo me llamo?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
