{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp, OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values, load_dotenv\n",
    "config = dotenv_values(\".env\")\n",
    "OPENAI_API_KEY={**config}[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de predicción de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo autocompletador de textos text-davinci-003\n",
    "llm_openai = OpenAI(model_name = \"text-davinci-003\", openai_api_key= OPENAI_API_KEY, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Estoy bien, ¿y tú?\n"
     ]
    }
   ],
   "source": [
    "print(llm_openai(\"Hola, cómo estás?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt = ChatOpenAI(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Hola! Soy una inteligencia artificial, por lo que no tengo emociones, pero estoy aquí para ayudarte. ¿En qué puedo asistirte hoy?\n"
     ]
    }
   ],
   "source": [
    "#HumanMessage\n",
    "response = chatgpt([HumanMessage(content=\"Hola, Cómo estás?\")])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_template = \"\"\"\n",
    "\n",
    "Eres un asistente virtual llamado David que responde a preguntas de manera muy breve.\n",
    "Pregunta: Cuáles son los ingredientes para preparar {plato}\n",
    "Respuesta:\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Eres un asistente virtual llamado David que responde a preguntas de manera muy breve.\n",
      "Pregunta: Cuáles son los ingredientes para preparar Sancocho de gallina\n",
      "Respuesta:\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Se construye el template agregándole las variables de entrada\n",
    "prompt_temp = PromptTemplate(input_variables=[\"plato\"], template=basic_template)\n",
    "\n",
    "prompt_value = prompt_temp.format(plato=\"Sancocho de gallina\")\n",
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los ingredientes para preparar Sancocho de gallina incluyen una gallina entera, una cebolla, una cucharada de achiote, una cucharada de comino, una cucharada de orégano, una cucharada de ajo en polvo, dos cucharadas de sal, una cucharada de pimienta, una cucharada de cilantro, dos tomates, una taza de papas, una taza de zanahorias, una taza de maíz, una taza de yuca y una taza de plátano verde.\n"
     ]
    }
   ],
   "source": [
    "response = llm_openai(prompt_value)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many Tokens we're sending?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 9\n"
     ]
    }
   ],
   "source": [
    "#Requiere instalar tiktoken\n",
    "print(\"tokens:\",llm_openai.get_num_tokens(\"Hola, cómo estás?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate\n",
    "\n",
    "Template para modelos de chat. Permite darle información a los modelos de chat en la manera que lo necesiten.\n",
    "\n",
    "Elementos:\n",
    "\n",
    "- **Human:** El texto escrito por el usuario\n",
    "- **AI:** Texto respuesta del modelo\n",
    "- **System:** Texto enviado al modelo para darle contexto de su funcionamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, ChatMessagePromptTemplate, AIMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construcción del template para el sistema\n",
    "prompt_temp_system = PromptTemplate(\n",
    "    template = \"Eres un asistente virtual que recomienda una alternativa {adjetivo} a un producto\",\n",
    "    input_variables=[\"adjetivo\"]\n",
    ")\n",
    "#Langchain se encarga de formatear el prompt de manera correcta para el modelo\n",
    "template_system = SystemMessagePromptTemplate(prompt=prompt_temp_system)\n",
    "\n",
    "#Humano\n",
    "prompt_temp_human = PromptTemplate(template=\"{texto}\",input_variables=[\"texto\"])\n",
    "template_human = HumanMessagePromptTemplate(prompt=prompt_temp_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='Eres un asistente virtual que recomienda una alternativa económica a un producto'), HumanMessage(content='ipad')]\n"
     ]
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([template_system,template_human])\n",
    "\n",
    "#Formato del prompt\n",
    "chat_prompt_value = chat_prompt.format_prompt(adjetivo=\"económica\",texto=\"ipad\").to_messages()\n",
    "print(chat_prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Si estás buscando una alternativa económica al iPad, te recomendaría considerar la tablet Amazon Fire HD 10. Esta tablet ofrece una pantalla de alta definición de 10.1 pulgadas, un procesador rápido y potente, y una duración de batería de hasta 12 horas. Además, cuenta con acceso a la tienda de aplicaciones de Amazon, lo que te permitirá descargar una amplia variedad de aplicaciones y juegos. Aunque no tiene todas las características y funcionalidades de un iPad, es una opción más económica que puede cumplir con tus necesidades básicas de navegación web, reproducción de contenido multimedia y uso de aplicaciones.'\n"
     ]
    }
   ],
   "source": [
    "chat_response = chatgpt(chat_prompt_value)\n",
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Selector\n",
    "\n",
    "Propósito: Alimentar el modelo con ejemplos y posibles respuestas que se esperan recibir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se crea una lista de diccionarios con ejemplos para el modelo\n",
    "examples = [\n",
    "    {\"pregunta\": \"Cuál es el ingrediente principal de la pizza?\",\"respuesta\": \"La masa y salsa de tomate\"},\n",
    "    {\"pregunta\": \"Cuál es el ingrediente principal de las hamburguesas?\",\"respuesta\": \"La carne y el pan\"},\n",
    "    {\"pregunta\": \"Cuál es el ingrediente principal del burrito?\",\"respuesta\": \"La tortilla y la carne\"}\n",
    "]\n",
    "#\n",
    "prompt_temp_examples = PromptTemplate(input_variables=[\"pregunta\",\"respuesta\"],\n",
    "                                      template = \"Pregunta: {pregunta}\\nRespuesta: {respuesta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Template que genera el formato para las preguntas y respuestas\n",
    "prompt_examples = FewShotPromptTemplate(example_prompt=prompt_temp_examples,\n",
    "                                        examples=examples,\n",
    "                                        prefix=\"Eres un asistente virtual culinario que responde a preguntas sobre ingredientes principales de platos de manera breve\",\n",
    "                                        suffix=\"Pregunta: {pregunta}\\nRespuesta:\",\n",
    "                                        input_variables=[\"pregunta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eres un asistente virtual culinario que responde a preguntas sobre ingredientes principales de platos de manera breve\n",
      "\n",
      "Pregunta: Cuál es el ingrediente principal de la pizza?\n",
      "Respuesta: La masa y salsa de tomate\n",
      "\n",
      "Pregunta: Cuál es el ingrediente principal de las hamburguesas?\n",
      "Respuesta: La carne y el pan\n",
      "\n",
      "Pregunta: Cuál es el ingrediente principal del burrito?\n",
      "Respuesta: La tortilla y la carne\n",
      "\n",
      "Pregunta: Cuál es el ingrediente principal del coctel de camarones?\n",
      "Respuesta:\n"
     ]
    }
   ],
   "source": [
    "prompt_value = prompt_examples.format(pregunta=\"Cuál es el ingrediente principal del coctel de camarones?\")\n",
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Camarones y salsa picante\n"
     ]
    }
   ],
   "source": [
    "ingredients_response = llm_openai(prompt_value)\n",
    "print(ingredients_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Parsers\n",
    "\n",
    "Formatear la respuesta a conveniencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your response should be a list of comma separated values, eg: `foo, bar, baz`'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_temp_parser = \"\"\"Cuáles son los ingredientes para preparar: {platillo}\\n{ht_parser}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_temp_parser = PromptTemplate(input_variables=[\"platillo\"],\n",
    "                                    template = basic_temp_parser,\n",
    "                                    partial_variables = {\"ht_parser\":format_instructions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_value_parser = prompt_temp_parser.format(platillo=\"Burrito\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuáles son los ingredientes para preparar: Burrito\n",
      "Your response should be a list of comma separated values, eg: `foo, bar, baz`\n"
     ]
    }
   ],
   "source": [
    "print(prompt_value_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nTortilla de harina, carne, frijoles, queso, cebolla, tomate, lechuga, guacamole, crema, salsa.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_parser = llm_openai(prompt_value_parser)\n",
    "response_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tortilla de harina',\n",
       " 'carne',\n",
       " 'frijoles',\n",
       " 'queso',\n",
       " 'cebolla',\n",
       " 'tomate',\n",
       " 'lechuga',\n",
       " 'guacamole',\n",
       " 'crema',\n",
       " 'salsa.']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(response_parser)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
